{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from pyvi import ViTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tập data chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tranning = ['./dữ liệu chatbot - dự án cuối kì/bạn bè.txt',\n",
    "'./dữ liệu chatbot - dự án cuối kì/các câu hỏi phức tạp.txt',\n",
    "'./dữ liệu chatbot - dự án cuối kì/đất nước.txt',\n",
    "'./dữ liệu chatbot - dự án cuối kì/địa chỉ.txt',\n",
    "'./dữ liệu chatbot - dự án cuối kì/du lịch.txt',\n",
    "'./dữ liệu chatbot - dự án cuối kì/gia đình.txt',\n",
    "'./dữ liệu chatbot - dự án cuối kì/giải trí.txt',\n",
    "'./dữ liệu chatbot - dự án cuối kì/học tập.txt',\n",
    "'./dữ liệu chatbot - dự án cuối kì/nghề nghiệp.txt',\n",
    "'./dữ liệu chatbot - dự án cuối kì/nghỉ lễ.txt',\n",
    "'./dữ liệu chatbot - dự án cuối kì/người yêu.txt',\n",
    "'./dữ liệu chatbot - dự án cuối kì/robot.txt',\n",
    "'./dữ liệu chatbot - dự án cuối kì/shoping.txt',\n",
    "'./dữ liệu chatbot - dự án cuối kì/shoping.txt',\n",
    "'./dữ liệu chatbot - dự án cuối kì/tán gẫu.txt',\n",
    "'./dữ liệu chatbot - dự án cuối kì/tdtu.txt',\n",
    "'./dữ liệu chatbot - dự án cuối kì/thông tin cá nhân.txt',\n",
    "'./dữ liệu chatbot - dự án cuối kì/trò chuyện về đi ăn.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_train = []\n",
    "answer_train = []\n",
    "for i in range(len(file_tranning)):\n",
    "    with open(file_tranning[i], encoding='UTF-8') as f:\n",
    "        train_lines = f.readlines()\n",
    "        for line in train_lines:\n",
    "            tmp = line.split(\"__eou__\")\n",
    "            question_train.append(tmp[0].strip()) # strip(): Loại bỏ whitespace đầu cuối string\n",
    "            answer_train.append(tmp[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(question_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thích đánh lộn không?\n",
      "Ngon nhà vô\n"
     ]
    }
   ],
   "source": [
    "print((question_train[0]))\n",
    "print((answer_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Segmentation \n",
    "for i in range(len(question_train)):\n",
    "    question_train[i] = ViTokenizer.tokenize(question_train[i])\n",
    "    answer_train[i] = ViTokenizer.tokenize(answer_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thích đánh_lộn không ?\n",
      "Ngon nhà_vô\n"
     ]
    }
   ],
   "source": [
    "print(question_train[0])\n",
    "print(answer_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tập vietnamese-stopwords-dash.txt\n",
    "# file_stopwords = './vietnamese-stopwords-dash.txt'\n",
    "\n",
    "# stopwords = []\n",
    "\n",
    "# with open(file_stopwords, encoding='UTF-8') as f:\n",
    "#     stopwords_lines = f.readlines()\n",
    "#     for line in stopwords_lines:\n",
    "#         stopwords.append(line.rstrip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "my_punctuation = (string.punctuation).replace(\"_\",\"\") # remove _ in my_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(my_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "#     STOPWORDS = stopwords\n",
    "    nopunc = [char for char in mess if char not in my_punctuation]\n",
    "#     nopunc = ''.join(nopunc)\n",
    "    return  ''.join(nopunc)\n",
    "#     return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bỏ dấu câu\n",
    "for i in range(len(question_train)):\n",
    "    question_train[i] = text_process(question_train[i])\n",
    "    answer_train[i] = text_process(answer_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thích đánh_lộn không \n",
      "Ngon nhà_vô\n"
     ]
    }
   ],
   "source": [
    "print(question_train[0])\n",
    "print(answer_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "sents=[]\n",
    "for i in range(len(question_train)):\n",
    "    tokens = word_tokenize(question_train[i])\n",
    "    sents.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "modelw2v = Word2Vec(sentences=sents, size=100, window=5, sg=1, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chuyển 1 câu thành 1 vector\n",
    "# vector đại diện cho câu theo phương pháp trung bình\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "def convert2vec(words):\n",
    "    sum_ = np.array([0]*100)\n",
    "    for word in words:\n",
    "        if(word not in modelw2v.wv):\n",
    "            continue\n",
    "        vec = modelw2v.wv[word]\n",
    "        sum_ = sum_ + vec\n",
    "    return sum_ / len(words) # Lấy vector của tất cả từ chia trung bình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(1 - spatial.distance.cosine(convert2vec(ViTokenizer.tokenize(\"Thích đánh lộn không?\")), convert2vec(question_train[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question):\n",
    "    question = ViTokenizer.tokenize(question)\n",
    "    question = convert2vec(question)\n",
    "    \n",
    "    cosin_lst = []\n",
    "    for i in range(len(question_train)):\n",
    "        cosin_lst.append(1 - spatial.distance.cosine(question, convert2vec(question_train[i])))\n",
    "#     print(cosin_lst)\n",
    "\n",
    "    question_pred = max(cosin_lst)\n",
    "    index = cosin_lst.index(question_pred)\n",
    "    return answer_train[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_input = \"1 + 1 bằng mấy?\"\n",
    "answer = get_answer(question_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  1 + 1 bằng mấy?\n",
      "A:  Bằng 2  Câu hỏi này dành cho học_sinh tiểu_học \n"
     ]
    }
   ],
   "source": [
    "print(\"Q: \", question_input)\n",
    "print(\"A: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: hehe\n",
      "A: chị học CNTT  con_gái học ngành này cũng thú_vị lắm  chị cũng có những người bạn tốt luôn giúp_đỡ nhau trong học_tập nên thấy cuộc_sống đại_học khá là thú_vị  nếu em thích học toán và đam_mê công_nghệ  đam_mê tìm_kiếm cái mới thì thử tìm_hiểu về CNTT nhé \n",
      "Q: TDTU\n",
      "A: Có gì đâu mà buồn  kiếm đứa khác thui  lấy đó làm động_lực phấn_đấu đi\n",
      "Q: bạn là ai ?\n",
      "A: hiện_tại mình học ngành khoa_học máy_tính\n",
      "Q: nhà bạn ở đâu?\n",
      "A: Q7\n",
      "Q: bạn tên gì?\n",
      "A: mình với người_yêu hay đi ăn rồi đi xem phim hay đi ra các điểm mà tụi mình thích\n",
      "Q: thua\n",
      "A: rồi ông\n",
      "Q: thế giới có bao nhiêu đại dương?\n",
      "A: Có  quê mình đi xe_máy khoảng 1 tiếng\n",
      "Q: hentai\n",
      "A: ok để sắp_xếp\n",
      "Q: bạn có người yêu chưa?\n",
      "A: mình đã làm gì có người_yêu  mình còn đang sợ ế đây này\n",
      "Q: stop\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question_input = input(\"Q: \")\n",
    "    if(question_input.strip() == \"stop\"):\n",
    "        break\n",
    "    print(\"A:\", get_answer(question_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X_tfidf = vectorizer.fit_transform(question_train)\n",
    "# print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
